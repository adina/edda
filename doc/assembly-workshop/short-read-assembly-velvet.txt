================================
Short Read Assembly with Velvet 
================================

:Author: Jason Pell
:Date: June 7, 2011
:Last Updated: December 11, 2011 (adina)

For this tutorial, we will be using the short read de novo assembler Velvet
to assemble a simulated metagenome. However, there are plenty of other assemblers 
out there (e.g. ABySS, ALLPATHS 2, SOAPdenovo, etc.) that may be more appropriate 
for your particular dataset, and we can help you get them installed if you 
are interested.

Getting Started
---------------

Start or log in to an instance on Amazon EC2 and note the number of CPUs (or
threads) available on your instance. You will need to have Dropbox 
set up (see :doc:`installing-dropbox`) and the data directory snapshot 
mounted (see 'Getting the data' at :doc:`fastq_tutorial`).

Go to your /mnt directory where there is more free space::

   %% cd /mnt

Next, we need to obtain some different files.  To obtain Velvet,
use curl::

  %% curl -O http://www.ebi.ac.uk/~zerbino/velvet/velvet_1.1.06.tgz

We're going to assemble a simulated metagenome which has been published 
in "Evaluating the Fidelity of De Novo Short Read Metagenomic Assembly Using
Simulated Data".  It is a mixture of simulated Illumina reads from 112 genomes.
(Pignatelli, 2011, PLoS One, http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3100316/)
Specifically, we'll be working with a subset partitioned from their high
complexity, high coverage dataset.  We're working with a nice small dataset
that should assemble pretty fast.  

Let's copy the dataset to your instance and uncompress it::

   %% cp /data/simulated_subset.fa .
   %% gunzip -d simulated_subset.fa.gzip 

We will also need some packages installed if this isn't already done::

   %% apt-get install make

Installing Velvet (threaded)
-----------------------------
Uncompress the tarballs and move into the Velvet directory::

  %% tar xvzf velvet_1.1.06.tgz
  %% cd velvet_1.1.06

Once you are in the Velvet directory, run the following to install::

  %% make 'MAXKMERLENGTH=49' 'OPENMP=2'

This command will compile the package. The MAXKMERLENGTH parameter is created as a 
tradeoff between memory efficiency and flexibility with the 'k' parameter. We have it 
set to be more than enough for our purposes, but you may need to set MAXKMERLENGTH 
higher depending on your dataset. The OPENMP parameters is the number of
threads which velvet will use for assembly.  This should be set according to
your computational resources (i.e. type of EC2 instance).

We will now "install" Velvet onto your instance::

   %% cp velveth /usr/bin
   %% cp velvetg /usr/bin

Running Velvet
--------------

Before you use velvet, you should read the manual
(http://www.ebi.ac.uk/~zerbino/velvet/).  You'll notice that velvet takes a
variety of input formats which you'll need to specify which ones you have.  Also, if you have
paired-end reads and non-paired end reads, you'll need to separate them (there
are several khmer scripts available to help out.)  Finally, if you've ran
Khmer to partition your data, your sequence IDs will be too long (as they
contain partition IDs) and you'll need to fix that before running velvet.
We've provided a file that should just work for simplicity.

Assembling with Velvet is a two-step process and the manual goes through the
specifics.  In summary, here's what you would typically do::

  %% velveth simHChc_subset.33 33 -fasta -short simulated_subset.fa
  %% velvetg simHChc_subset.33 -exp_cov auto -cov_cutoff 0 -scaffolding no

The simHChc_subset.33 is the name of the directory which will contain the
velvet output including the final contigs.  The 33 is the value that we are 
selecting for the K parameter. This means that we are using 31-mers to look for overlaps between the 
reads. This value depends on your "sensitivity" and "specificity" needs, and
we'll discuss this.

Evaluating Assemblies
---------------------
Assessing the quality of a de novo assembly where there is no good
reference is still an open problem.  However, there are many
statistics that can be useful in comparing assemblies to one another.
We have a Python script that provides many of these statistics; it's
in the ngs-course scripts, under 'ngs-course/assembly/assemstats3.py'.

Before you can run it, you must install screed, so we'll run the
following in the home directory::

  %% git clone git://github.com/ctb/screed.git
  %% cd screed
  %% python setup.py install

Screed is a software package that was developed to easily parse and
manipulate a large amount of short read sequences.  Once the reads are
indexed, you can very quickly retrieve any sequence you need by name.
However, the assembly statistics Python script just uses the provided
FASTA parser.

To obtain statistics on the file you just generated, run::

  %% cd /mnt
  %% python ~/Dropbox/ngs-scripts/assembly/assemstats3.py 0 k*/contigs.fa

Other Stuff To Try
------------------
The optimal value for k depends greatly on the dataset.  A lower value
for k has greater sensitivity, but can produce more false overlaps.
However, it is the best option when you don't have high coverage.  On
the other hand, a high value for k will have a more accurate assembly
and longer contigs, but you are likely to miss a lot of potential read
overlaps, which means you need higher coverage to make up for the
difference.

Try varying the value for k by creating a new directory for each value
that you would like to test. With Velvet, you can only choose odd number 
k-mers. Furthermore, you must choose a k that is smaller than the read length, 
so in this case k < 36. After you have created all of your assemblies, run::

  %% python ~/Dropbox/ngs-scripts/assembly/assemstats3.py 0 k*/contigs.fa

again to see how the assemblies compare to each other.  If you
generate a lot of assemblies, you can copy and paste the output to a
text file and import it into Excel as a space-delimited file.

We have another script for you to try called extract-long-sequences.fa. 
This script acts as a simple filter to get rid of sequences smaller than a 
certain length. For example, if you have a file named contigs.fa and you 
want only contigs of length 200 or greater, you can run::

   %% python ~/Dropbox/ngs-scripts/assembly/extract-long-sequences.py 200 contigs.fa > out.fa

Finally, if you have your own dataset, you can try to assemble it on
your EC2 system.  However, you may want to check memory usage with the
top command in order to ensure that you are not using too much memory.
If the Velvet memory usage is over 90%, you may be using virtual
memory, which means the assembly will take much longer than necessary.
